Evaluating Generative AI Drumbeats on Guitar Tracks

The challenge of generating expressive and coherent drum accompaniments remains a significant barrier for solo guitarists and independent musicians. Traditional solutions such as DAW-based programming or loop libraries are either time-consuming or musically generic, often failing to reflect the intricacies of a guitaristâ€™s performance. Prior research in this space primarily utilizes symbolic MIDI data and sequence models like RNNs or Transformers to produce drum tracks, but these often suffer from repetitive patterns, weak structural awareness, and limited audio fidelity.

Our project aims to address these limitations by leveraging a hybrid architecture that combines Self-Similarity Matrices (SSMs), Transformer networks, and Diffusion Models to generate realistic and stylistically responsive drum accompaniments from guitar tracks. Unlike previous models, which separately predict rhythm and dynamics or rely on MIDI-only input, our multitask system jointly learns to predict both the drum SSM and the corresponding drum Mel-spectrogram, enhancing temporal precision and musical alignment.

The dataset was constructed by performing source separation on 710 publicly available multitrack metal and rock songs using Demucs, yielding paired guitar and drum audio. Mel-spectrograms and SSMs were extracted using Short-Time Fourier Transform (STFT) and cosine similarity for feature representation. The Transformer was trained in a multitask setup using cross-attention layers to condition drum generation on both guitar structure and timbre. A diffusion model was then used to reconstruct the drum audio waveform from the predicted SSM, constrained by learned structural priors.

